# -*- coding: utf-8 -*-
"""EGBI222_Group_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j1xnBLIfG4mI8ymLQfJ3KEinHNPCfASU

**Set up the program to use**
"""

!pip install kaggle
from google.colab import drive
drive.mount('/content/drive/')

# 1. Remove the file (force delete, even if it's not a folder)
!rm -rf /root/.kaggle

# 2. Recreate as a directory
!mkdir -p /root/.kaggle

# 3. Copy kaggle.json from Drive
!cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle/

# 4. Fix permissions
!chmod 600 /root/.kaggle/kaggle.json

!pip install yt-dlp
!pip install -q git+https://github.com/openai/whisper.git
!apt-get install -y ffmpeg
!pip uninstall torch torchvision torchaudio -y
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install faster-whisper

!pip install -q -U google-generativeai
!pip install -q langdetect
# Import the Python SDK
import google.generativeai as genai
# Used to securely store your API key
#from google.colab import userdata
import os
#GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)
model_gemini = genai.GenerativeModel('gemini-2.5-flash')
!pip install deep-translator

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import yt_dlp
import shutil
import time, random, pandas as pd, yt_dlp
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm.notebook import tqdm
import torch
from faster_whisper import WhisperModel
import whisper
import torch
from whisper.utils import get_writer
from langdetect import detect
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay

!kaggle datasets list
!kaggle datasets download cyberevil545/youtube-videos-data-for-ml-and-trend-analysis
!unzip /content/youtube-videos-data-for-ml-and-trend-analysis.zip

"""**Import file + clean file**"""

df = pd.read_csv('/content/drive/MyDrive/EGBI222_Group Project_1/youtube_data.csv')
df.sort_values(by="duration", ascending=False)
df = df[(df['duration'] > 60) & (df['duration'] < 600)]
df

"""**Programs for each machine**

P=0, Raphael=1,2 ,chen=3,ice=4
"""

#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà) ‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡πÄ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡πÄ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏ô
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâModulo ‡πÅ‡∏¢‡∏Å‡πÄ‡πÄ‡∏ö‡∏ö‡∏´‡∏≤‡∏£‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà‡∏Ñ‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏≥‡∏≠‡∏±‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏≤‡∏ÅIndex
CSV_PATH      = "/content/drive/MyDrive/EGBI222_Group Project_1/youtube_data.csv"   # ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
OUTPUT_DIR    = "/content/drive/MyDrive/EGBI222_Group Project_1/audio"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö .mp3
RESULTS_DIR   = "/content/drive/MyDrive/EGBI222_Group Project_1/results"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö CSV ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

TOTAL_MACHINES = 5     # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á(‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡∏±‡∏ô)
MACHINE_ID     = 1    # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á(‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô).                                           #<-------------------------------------------‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ

# ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ
SUFFIX = f"part_name"                                     #<-------------------------------------------‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô name ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
OUT_CSV = os.path.join(RESULTS_DIR, f"processed_{SUFFIX}.csv")

MAX_WORKERS_DL = 3
SLEEP_RANGE    = (0.5, 1.5)
COOKIE_PATH = None          #‡∏ñ‡πâ‡∏≤‡∏ü‡∏≤‡∏´‡∏≤‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ‡πÄ‡∏à‡∏≠‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

if "url" not in df.columns:
    raise ValueError( "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'url' ‡πÉ‡∏ô CSV")

# ‡πÉ‡∏™‡πà‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô
for col in ["audio_path", "Transcript", "Translate"]:
    if col not in df.columns:
        df[col] = None

#‡πÅ‡∏ö‡πà‡∏á‡∏à‡∏≤‡∏Å index
work_df = df[df.index % TOTAL_MACHINES == MACHINE_ID].copy().reset_index(drop=True)

display(work_df)
print(f"‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(work_df)} ‡πÅ‡∏ñ‡∏ß") #‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì2200-2300

"""***Download(Video)***"""

#‡πÇ‡∏Ñ‡πä‡∏î‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà ‡∏Å‡πá‡∏≠‡∏õ‡∏°‡∏≤ ‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏á‡∏ô‡πâ‡∏≤
#‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏°‡∏±‡∏ô‡∏°‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Ådef ‡∏ó‡∏µ‡∏•‡∏∞‡∏£‡∏≠‡∏ö‡∏°‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏µ‡∏•‡∏∞‡∏≠‡∏±‡∏ô‡∏°‡∏±‡∏ô‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ä‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏ô‡∏∞
#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà) ‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏≠‡∏µ‡∏Å‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ä‡∏∑‡πà‡∏≠(Quick Download)
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤def‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏î‡∏µ‡πÄ‡πÄ‡∏•‡πâ‡∏ß ‡πÄ‡πÄ‡∏ï‡πà‡πÑ‡∏õ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏•‡∏≤‡∏¢‡πÜ‡∏≠‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏ß‡πÜ ‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°skip‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡πÄ‡πÄ‡∏•‡πâ‡∏ß(‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡∏ß‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏±‡∏ô‡∏°‡∏±‡∏ô‡πÄ‡∏ú‡∏•‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏î‡πâskip)
def download_audio_files(youtube_url, video_id):
    output_dir = "/content/drive/MyDrive/EGBI222_Group Project_1/audio"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    audio_file_path = os.path.join(output_dir, f'{video_id}.mp3')

    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': audio_file_path.replace('.mp3', ''),
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
        }],
        'quiet': True,
        'no_warnings': True,
    }

    print(f"üéß Attempting to download: {youtube_url}")
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([youtube_url])
        print(f"‚úÖ Successfully downloaded: {audio_file_path}")
        return audio_file_path
    except yt_dlp.utils.DownloadError as e:
        print(f"‚ö†Ô∏è ERROR downloading {youtube_url} (ID: {video_id}). Skipping file.")
        return None
    finally:
        time.sleep(2)

df_test = work_df.iloc[:5]#<------------------- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ

work_df["audio_path"] = df_test.apply(
    lambda row: download_audio_files(row['url'], row['video_id']),
    axis=1
  )

""" **Quick Download**"""

from concurrent.futures import ThreadPoolExecutor, as_completed
import yt_dlp, time, os
def download_audio_files(youtube_url, video_id):
    output_dir = "/content/drive/MyDrive/EGBI222_Group Project_1/audio"
    os.makedirs(output_dir, exist_ok=True)

    audio_file_path = os.path.join(output_dir, f"{video_id}.mp3")

    if os.path.exists(audio_file_path):
        print(f"‚è© Skip {video_id} (already exists)")
        return audio_file_path

    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': audio_file_path.replace('.mp3', ''),
        'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3'}],
        'quiet': True,
        'no_warnings': True,
    }

    try:
      #‡∏ü‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡πÉ‡∏™‡πà‡∏≠‡∏µ‡πÇ‡∏°‡∏à‡∏¥‡∏°‡∏µ‡∏™‡∏µ‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡πâ‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÜ( Àò ¬≥Àò)‚ô•
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([youtube_url])
        print(f"‚úÖ Done: {video_id}")
        return audio_file_path
    except Exception as e:
        print(f"‚ö†Ô∏è Error downloading {video_id}: {e}")
        return None
    finally:
        time.sleep(1) #‡πÄ‡∏≠‡∏≤‡∏ä‡∏±‡∏ß‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏±‡∏ô‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏û‡∏±‡∏Å‡∏ö‡πâ‡∏≤‡∏á

#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà)‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÄ‡πÄ‡∏ö‡∏ö‡∏Ç‡∏ô‡∏≤‡∏ô‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
#‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡∏£‡∏±‡∏ô‡∏£‡∏±‡∏ß‡πÜ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâyoutube‡∏à‡∏∞‡∏´‡∏≤‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏≠‡∏ó(time.sleep‡∏Å‡πá‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏∞‡∏•‡∏≠‡∏á‡πÄ‡πÄ‡∏¢‡πâ‡∏ß) #‡πÇ‡∏î‡∏ô‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏ï‡∏≠‡∏ô315-350
df_test = work_df.iloc[:300]  # ‚Üê ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ #‡∏Ñ‡∏¥‡∏î‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÄ‡πÄ‡∏¢‡πâ‡∏ß‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡∏•‡∏∞300‡πÑ‡∏õ‡∏•‡∏∞‡∏Å‡∏±‡∏ô‡∏ô‡∏∞  ‚îë(Ôø£‚ñΩÔø£)‚îç
max_workers = 5 #5‡∏Å‡∏±‡∏ö10 ‡∏î‡∏µ‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï ‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏°‡∏≤‡∏•‡∏∞

futures = []
paths = [None] * len(df_test)

with ThreadPoolExecutor(max_workers=max_workers) as executor:
    for i, row in enumerate(df_test.itertuples(index=False)):
        futures.append(executor.submit(download_audio_files, row.url, row.video_id))

    for i, fut in enumerate(as_completed(futures)):
        paths[i] = fut.result()

# ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ú‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ work_df (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î)
work_df.loc[df_test.index, "audio_path"] = paths
print("‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ßüôÉ")

"""**##########################################################**

**Remove unloaded files**
"""

#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà)‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñpath(‡∏•‡∏á‡πÉ‡∏ôcolumn["audio_path"]) ‡∏•‡∏∞‡∏≠‡∏±‡∏ô‡πÑ‡∏´‡∏ô‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡πÄ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏•‡∏ö‡∏ó‡∏¥‡πâ‡∏á
def check_audio_path(video_id):

    audio_file_path = os.path.join(OUTPUT_DIR, f"{video_id}.mp3")
    if os.path.exists(audio_file_path):
        return audio_file_path
    else:
        return None


work_df["audio_path"] = work_df["video_id"].apply(check_audio_path)

work_df.dropna(subset=['audio_path'], inplace=True)
display(work_df)
print(f"‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(work_df)} ‡πÅ‡∏ñ‡∏ß")

"""**#######################‡∏´‡πâ‡∏≤‡∏°‡∏Å‡∏î‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ñ‡πâ‡∏≤‡∏£‡∏≤‡∏ü‡∏≤‡πÄ‡∏≠‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡∏±‡πà‡∏á ##########################**"""

#‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ñ‡∏∂‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ô‡∏µ‡πâ‡∏ó‡∏±‡∏Å‡πÑ‡∏•‡∏ô‡πå‡∏£‡∏≤‡∏ü‡∏≤‡πÄ‡∏≠‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏ó‡∏£0982560366 (‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏≤‡∏à‡∏ï‡∏≠‡∏ö‡∏ä‡πâ‡∏≤)
#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà)‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡πÄ‡∏£‡∏Å‡∏•‡∏ádrive(‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏¢)*‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏à‡∏≤‡∏Å‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï
import pandas as pd, os
RESULTS_DIR = "/content/drive/MyDrive/EGBI222_Group Project_1/results"
os.makedirs(RESULTS_DIR, exist_ok=True)
OUT_CSV = os.path.join(RESULTS_DIR, "progress_name.csv") #<-------------------------------------------‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô name ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ

work_df.to_csv(OUT_CSV, index=False, encoding="utf-8")
print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡πâ‡∏ß -> {OUT_CSV} ({len(df)} ‡πÅ‡∏ñ‡∏ß)")

"""**####################################################################**

**Load Save**
"""

#Run time out
#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà)‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Åprogress_name.csv ‡∏à‡∏≤‡∏Ådrive ‡∏à‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏ã‡∏ü‡∏°‡∏≤‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ ‡∏°‡∏±‡∏ô‡∏´‡∏•‡∏∏‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏¢(‡∏Ñ‡∏≠‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏à‡∏∞‡∏û‡∏±‡∏á‡∏ï‡∏≠‡∏ô‡πÑ‡∏´‡∏ô‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏î‡∏µ‡∏™‡∏∏‡∏îüëç)
import pandas as pd, os

RESULTS_DIR = "/content/drive/MyDrive/EGBI222_Group Project_1/results"
OUT_CSV = os.path.join(RESULTS_DIR, "progress_name.csv")#<-------------------------------------------‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô name ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ

if os.path.exists(OUT_CSV):
    work_df = pd.read_csv(OUT_CSV)
    print(f"‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏° -> {OUT_CSV} ({len(work_df)} ‡πÅ‡∏ñ‡∏ß)")
#‡πÄ‡∏ä‡πá‡∏Ñpath‡πÄ‡∏≠‡∏≤‡∏ä‡∏±‡∏ß ‡∏£‡∏±‡∏ô‡πÉ‡∏ô‡πÄ‡∏ã‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Å‡∏î‡πÄ‡πÄ‡∏Ñ‡πà‡∏≠‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß(‡∏Å‡∏•‡∏±‡∏ß‡∏û‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏Å‡∏î‡∏≠‡∏±‡∏ô‡∏ö‡∏ô‡∏•‡∏∞‡∏à‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏îüôÉ)
def check_audio_path(video_id):
    Merged = "/content/drive/MyDrive/EGBI222_Group Project_1/audio"
    audio_file_path = os.path.join(Merged, f"{video_id}.mp3")
    if os.path.exists(audio_file_path):
        return audio_file_path
    else:
        return None
work_df["audio_path"] = work_df["video_id"].apply(check_audio_path)
work_df.dropna(subset=['audio_path'], inplace=True)
display(work_df)

"""**Transcipt+Translate**"""

#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà)‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ôTranscipt+Translate ‡∏£‡∏µ‡πÄ‡∏ó‡∏£‡∏¥‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
device = "cuda" if torch.cuda.is_available() else "cpu"
compute = "float16" if device == "cuda" else "int8"
MODEL_SIZE = "small"           #‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•Transcipt‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
model = WhisperModel(MODEL_SIZE, device=device, compute_type=compute)

def transcribe_one(path: str) -> str:
    segments, _ = model.transcribe(
        path,
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=500), #‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÅ‡∏ö‡∏ö 100 ‡∏Ñ‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÄ‡∏á‡∏µ‡∏¢‡∏ö100ms ‡πÉ‡∏´‡πâ‡∏à‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ô‡∏±‡πâ‡∏ô 500 ‡∏î‡∏µ‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï ‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏°‡∏≤‡∏•‡∏∞
        beam_size=1, best_of=1,
        without_timestamps=True,
        word_timestamps=False,
        condition_on_previous_text=False,
        temperature=0
    )
    return "".join(s.text for s in segments).strip()

def translate_one(path: str) -> str:
    # Whisper translate ‡πÅ‡∏õ‡∏• "‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (task=translate)
    segments, _ = model.transcribe(
        path,
        task="translate",
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=500), #‡πÅ‡∏õ‡∏•‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏Å‡πá‡πÑ‡∏î‡πâ(‡∏Å‡πá‡∏≠‡∏õ‡∏°‡∏≤‡∏à‡∏≤‡∏ÅtranscribeüôÉ)
        beam_size=1, best_of=1,
        without_timestamps=True,
        word_timestamps=False,
        condition_on_previous_text=False,
        temperature=0
    )
    return "".join(s.text for s in segments).strip()
#import ‡πÉ‡∏´‡∏°‡πà colab ‡∏°‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£‡πÑ‡∏°‡πà‡∏£‡∏π‡πâüò≠
import os, re
from tqdm.notebook import tqdm
#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà)‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏•‡∏¢‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏•‡∏¥‡∏õ‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ö‡∏ó‡∏û‡∏π‡∏î‡∏Å‡πá‡∏•‡∏ö‡∏ó‡∏¥‡πâ‡∏á‡πÄ‡∏•‡∏¢‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏õclean‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á(ML ‡πÄ‡∏î‡∏≤Category‡∏à‡∏≤‡∏Åtranslate(‡∏Ñ‡∏•‡∏¥‡∏õ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ö‡∏ó‡∏û‡∏π‡∏î‡∏°‡∏±‡∏ô‡∏à‡∏∞‡πÄ‡∏î‡∏≤‡∏°‡∏±‡πà‡∏ß))
def is_valid_sentence(s: str, min_chars=10, min_words=2) -> bool:
    if not isinstance(s, str):
        return False
    s = s.strip()
    if not s:
        return False
    if s.upper().startswith("ERROR"):
        return False
    if len(s) < min_chars:
        return False
    # ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ñ‡∏£‡πà‡∏≤‡∏ß ‡πÜ (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡πÄ‡∏•‡∏¢ ‡∏à‡∏∞‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏û‡∏≠) (‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ô‡πâ‡∏≠‡∏¢‡πÑ‡∏õ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâML ‡πÄ‡∏î‡∏≤‡∏¢‡∏≤‡∏Å‡∏•‡∏∞‡πÄ‡∏î‡∏≤‡∏ú‡∏¥‡∏î)
    words = re.findall(r'\w+', s, flags=re.UNICODE)
    if len(words) < min_words and len(s) < (min_chars * 1.5):
        return False
    return True

#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà)‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏£‡∏±‡∏ô‡πÄ‡πÄ‡∏ö‡∏ö‡∏£‡∏±‡∏ô‡πÑ‡∏õ‡πÄ‡∏ã‡∏ü‡πÑ‡∏õ ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏¢
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâwhile loop ‡πÄ‡πÄ‡∏ö‡∏ö‡∏£‡∏±‡∏ô‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ‡πÄ‡∏•‡∏¢‡∏à‡∏ô‡∏´‡∏°‡∏î ‡∏ñ‡πâ‡∏≤‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏û‡∏±‡∏á‡∏Å‡πá‡πÑ‡∏õ‡∏£‡∏±‡∏ô‡∏ï‡∏£‡∏áRun time out ‡πÉ‡∏´‡∏°‡πà‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏≠‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏•‡∏¢
import time
from tqdm.notebook import tqdm
import os

BATCH_SIZE = 10 #‡πÄ‡∏ã‡∏ü‡∏ó‡∏∏‡∏Å‡πÜ‡∏Å‡∏µ‡πà‡∏≠‡∏±‡∏ô ‡∏ô‡πâ‡∏≠‡∏¢‡πÑ‡∏õ‡∏Å‡πá‡∏ä‡πâ‡∏≤ ‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á‡∏Å‡πá‡∏£‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏∞

while True:

    pending = work_df[work_df["Transcript"].isna() | (work_df["Transcript"].astype(str).str.strip() == "")]
    if len(pending) == 0:
        print("‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß‡ºº ‚óî Õú ñ ‚óî ‡ºΩ")
        break

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å batch 10 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠
    df_subset = pending.iloc[:BATCH_SIZE]
    print(f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥ {len(df_subset)} ‡πÅ‡∏ñ‡∏ß ‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {len(pending)} ‡πÅ‡∏ñ‡∏ß")

    pbar = tqdm(total=len(df_subset), desc="‡∏Å‡∏≥‡∏•‡∏±‡∏áTranscipt+Translate", unit="file")
    drop_indices = [] #‡πÄ‡∏Å‡πá‡∏ö‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏•‡∏∞‡∏•‡∏ö‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß(‡πÑ‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠‡∏•‡∏∞‡∏•‡∏ö‡πÜ)O(1)‡∏Å‡∏±‡∏öO(n)
    for i, row in df_subset.iterrows():
        path = row["audio_path"]

        if not isinstance(path, str) or not os.path.exists(path):
            drop_indices.append(i)
            pbar.update(1)
            continue

        # ‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á
        txt = transcribe_one(path)
        if not is_valid_sentence(txt):
            drop_indices.append(i)
            pbar.update(1)
            continue

        # ‡∏ñ‡∏≠‡∏î‡πÑ‡∏î‡πâ ‚Üí ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á df
        work_df.at[i, "Transcript"] = txt

        # ‡πÅ‡∏õ‡∏•
        trans = translate_one(path)
        work_df.at[i, "Translate"] = trans

        pbar.update(1)

    if drop_indices:
        work_df.drop(index=drop_indices, inplace=True)
        work_df.reset_index(drop=True, inplace=True)
        print(f"‡∏•‡∏ö {len(drop_indices)} ‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô")

    pbar.close()

    # ‚úÖ ‡πÄ‡∏ã‡∏ü‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°
    work_df.to_csv(OUT_CSV, index=False, encoding="utf-8")
    print(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏° -> {OUT_CSV}")

    # ---- ‡∏û‡∏±‡∏Å‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á batch ----
    time.sleep(3) #‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏û‡∏±‡∏Å‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÄ‡∏≠‡∏≤‡∏ä‡∏±‡∏ß‡∏£‡πå

display(work_df)

"""**Save**


"""

#‡πÄ‡∏ã‡∏ü‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏≠‡∏≤‡∏ä‡∏±‡∏ß‡∏£‡πå
work_df.to_csv(OUT_CSV, index=False, encoding="utf-8")
print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß: {OUT_CSV}")

"""**Merge files**

*   ‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏•‡∏á‡πÉ‡∏ôDrive results ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏ü‡∏≤ (‡∏™‡πà‡∏á‡πÄ‡πÄ‡∏•‡πâ‡∏ß‡∏ö‡∏≠‡∏Å‡∏£‡∏≤‡∏ü‡∏≤‡∏î‡πâ‡∏ß‡∏¢)
*   ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡πÄ‡πÄ‡∏•‡πâ‡∏ß(‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡πÄ‡πÄ‡∏•‡πâ‡∏ß)
"""

from google.colab import drive
drive.mount('/content/drive/')

#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà)‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå ‡∏•‡∏∞‡πÄ‡∏≠‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡∏ó‡∏µ‡πà folder Master
import pandas as pd
import glob

# ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
path = "/content/drive/MyDrive/EGBI222_Group Project_1/results"
Master_path = "/content/drive/MyDrive/EGBI222_Group Project_1/Master"

all_files = glob.glob(path + "/*.csv")
dfs = [pd.read_csv(f) for f in all_files]
merged_df = pd.concat(dfs, ignore_index=True)
merged_df = merged_df.drop_duplicates(keep='last')
# ‡∏î‡∏π‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏ß‡∏°
print("‡∏£‡∏ß‡∏°‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", len(merged_df), "‡πÅ‡∏ñ‡∏ß")

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà
merged_df.to_csv(Master_path + "/Master.csv", index=False)
display(merged_df)
merged_df.shape

#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà)‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏áclean file ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà(‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏±‡∏ö‡∏≠‡∏±‡∏ô‡πÄ‡∏Å‡πà‡∏≤)
import pandas as pd

# ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå
path = "/content/drive/MyDrive/EGBI222_Group Project_1/Master/Master.csv"
df = pd.read_csv(path)

# ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥
print("‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î:", df.shape)
print(df.columns)

df = df.dropna(subset=['Translate', 'category'])
df = df[
    (df['Transcript'].astype(str).str.strip() != '') &
    (df['Translate'].astype(str).str.strip() != '') &
    (df['category'].astype(str).str.strip() != '')
]

df['Translate'] = df['Translate'].astype(str).str.strip()
df['category'] = df['category'].astype(str).str.strip()

df = df.drop_duplicates(subset=['Transcript', 'Translate', 'category'], keep='last')
#‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï index
df = df.reset_index(drop=True)
# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà
clean_path = "/content/drive/MyDrive/EGBI222_Group Project_1/Master/Master_clean.csv"
df.to_csv(clean_path, index=False)

print("‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î:", df.shape)
print("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà:", clean_path)

"""**Train ML**"""

#(‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà)‡∏ü‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏°‡∏≤train ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏î‡∏≤category‡∏à‡∏≤‡∏Å‡πÄ‡πÄ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‡πÄ‡∏ß‡πâ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥ ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: 5380 ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏°‡∏ß‡∏î: 16
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤ ‡∏à‡∏∞‡∏ï‡∏±‡∏îcategory ‡∏ô‡πâ‡∏≠‡∏¢‡πÜ‡∏≠‡∏≠‡∏Å ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡πÄ‡∏Ç‡πâ‡∏≤other
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏£‡∏ß‡∏°other ‡πÄ‡πÄ‡∏•‡πâ‡∏ßML ‡∏°‡∏±‡∏ô‡πÑ‡∏õ‡∏ï‡∏≠‡∏öother‡∏ã‡∏∞‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏•‡∏¢
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÄ‡∏•‡∏¢‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏•‡∏≠‡∏á‡∏ï‡∏±‡∏î‡πÄ‡πÄ‡∏•‡πâ‡∏ß‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤ 6 ‡πÑ‡∏î‡πâ%‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î ‡πÄ‡πÄ‡∏ï‡πà‡∏´‡∏°‡∏ß‡∏î‡∏à‡∏∞‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏¢‡πÑ‡∏õ ‡∏à‡∏≤‡∏Å16‡∏´‡∏°‡∏ß‡∏î ‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏ß‡∏£‡πÑ‡∏°‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤8
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏•‡∏≠‡∏á‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏°‡∏ß‡∏î‡πÄ‡πÄ‡∏•‡πâ‡∏ßtrain ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ôplot ‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡πÄ‡∏•‡πâ‡∏ß8-9‡∏´‡∏°‡∏ß‡∏î‡∏î‡∏µ‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏•‡∏á‡πÄ‡πÄ‡∏•‡πâ‡∏ß
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏•‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡πÄ‡∏•‡πâ‡∏ßtrain ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ôplot ‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡πÄ‡∏•‡πâ‡∏ß ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì2500-3200 ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö‡∏´‡∏°‡∏ß‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏û‡∏≠‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÄ‡πÄ‡∏•‡πâ‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà38%
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ôML ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà RoBARTa(DistilBERT) 41.2%(‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á linear ,TF-IDF Vectorizer(LogisticRegression), solver,DistilBERT,MiniLM(,MiniLM-L16))
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏ó‡∏£‡∏ôML ‡πÄ‡πÄ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâML‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâtrain‡∏ã‡πâ‡∏≠‡∏ô‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•Best of 1000row‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡πÉ‡∏´‡πâML‡∏°‡∏µacc‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏•‡∏∞‡πÄ‡∏≠‡∏≤‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏≤‡πÄ‡∏ó‡∏£‡∏ôML‡πÉ‡∏´‡∏°‡πà
#Best of 1000row ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ôrange 315‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ‡∏û‡∏≠‡∏î‡∏µ(‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á.str.split().str.len() >= 315 ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢) ‡πÇ‡∏î‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà 1350 ‡πÇ‡∏î‡∏¢ ‡∏´‡∏°‡∏ß‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà ‡πÄ‡πÄ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤60 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
#‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà LogisticRegression 43.2%
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï ‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏´‡∏≤Range best of 1000 ‡∏Ç‡∏≠‡∏á‡πÄ‡πÄ‡∏ï‡πà‡∏•‡∏∞model ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÄ‡πÄ‡∏•‡πâ‡∏ß ‡∏ô‡∏≥‡∏°‡∏≤‡∏¢‡∏π‡πÄ‡∏ô‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏±‡∏ô‡∏°‡∏µBest of 1000 ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì1078 ‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤2model
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏•‡∏≠‡∏á‡πÄ‡∏≠‡∏≤1078‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏£‡∏±‡∏ô‡∏•‡∏∞‡πÑ‡∏î‡πâ   LogisticRegression 44.1%
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏•‡∏≠‡∏á‡∏´‡∏≤test_size ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î(‡πÇ‡∏î‡∏¢‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤0.2)‡∏Ç‡∏≠‡∏á‡πÄ‡πÄ‡∏ï‡πà‡∏•‡∏∞model ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ôplot ‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡πÄ‡∏•‡πâ‡∏ß(‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å16‡∏ä‡∏°. (‚îÄ‚Äø‚Äø‚îÄ) )
#‡πÑ‡∏î‡πâlinear(0.28-0.33(‡πÅ‡∏ï‡πàacc‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å(‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤30%))) ,TF-IDF Vectorizer(LogisticRegression)(0.3-0.33), solver(0.27-0.31),DistilBERT(0.24-0.26),MiniLM(,MiniLM-L16(0.29-0.32)) ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà  LogisticRegression 44.1%(0.33)
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï ‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏´‡∏≤‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å1078 0kd ‡πÉ‡∏ä‡πâML‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâtrain‡∏ã‡πâ‡∏≠‡∏ô‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ ‡∏°‡∏≤‡πÉ‡∏™‡πà‡πÉ‡∏ôLogisticRegression (0.33)‡∏î‡∏π
#‡∏•‡∏≠‡∏á‡πÑ‡∏î‡πâ998 ‡∏ä‡∏∏‡∏î‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç(.str.split().str.len() >= 315 & category_counts < 70) ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà  LogisticRegression 49%(0.33) (‡∏≠‡∏±‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡πÄ‡∏•‡∏¢) (‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏≠‡∏µ‡∏Å‡∏•‡∏∞ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏≥(‡∏à‡∏£‡∏¥‡∏á‡πÜ‡∏Ç‡∏Å.‡∏•‡∏∞‡∏≠‡πà‡∏∞‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢))
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏à‡∏∞‡∏•‡∏≠‡∏á ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö‡∏´‡∏°‡∏ß‡∏î ‡πÄ‡πÄ‡∏•‡πâ‡∏ß‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü‡∏î‡∏π(‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å11‡∏ä‡∏°. (‚îÄ‚Äø‚Äø‚îÄ) )(‡∏ó‡πâ‡∏≠‡πÄ‡πÄ‡∏¢‡πâ‡∏ß‡∏ô‡πâ‡∏≤‡∏≤‡∏≤‡∏≤‡∏≤‡∏≤(‚Ä¢Ôπè‚Ä¢) )
#‡πÑ‡∏î‡πâ1023‡∏ä‡∏∏‡∏î‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç(.str.split().str.len() >= 315  .str.split().str.len() <= 1750 & category_counts < 63) (‡∏à‡∏£‡∏¥‡∏á‡πÜ1026 ‡πÄ‡πÄ‡∏ï‡πà‡∏´‡∏≤‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ(‡∏à‡∏£‡∏¥‡∏á‡πÜ‡∏¢‡∏±‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ‡∏ô‡∏∞‡πÄ‡πÄ‡∏ï‡πà‡∏Ç‡∏Å.'‚ó°' ))
#‡∏•‡∏≠‡∏á LogisticRegression 51.1%
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏•‡∏≠‡∏á‡∏´‡∏≤test_size ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î(‡πÇ‡∏î‡∏¢‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤0.2)‡∏Ç‡∏≠‡∏áLogisticRegression ‡∏Å‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏î‡∏π‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ôplot‡∏Å‡∏£‡∏≤‡∏ü‡∏´‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î((‡∏Ç‡∏ô‡∏≤‡∏î‡∏≠‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏¢‡∏±‡∏á‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö4‡∏ä‡∏°. (‚îÄ‚Äø‚Äø‚îÄ) ))
#‡πÑ‡∏î‡πâ0.27-0.35
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï ‡∏•‡∏≠‡∏á‡∏•‡∏∞‡πÑ‡∏î‡πâ0.31 51.5%
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏•‡∏≠‡∏á 1026 ‡∏ä‡∏∏‡∏î‡∏°‡∏≤‡πÄ‡∏ó‡∏£‡∏ô ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° 48.7% (‡∏•‡∏≠‡∏á‡∏Å‡∏î‡πÜ‡∏°‡∏∑‡∏≠‡∏´‡∏≤‡∏•‡∏∞‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏Å‡∏™‡∏∏‡∏î‡∏•‡∏∞(‡∏Ç‡∏Å.‡∏£‡∏±‡∏ô‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡πÄ‡∏¢‡πâ‡∏ß‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô))
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠‡πâ‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ 51.5%
#‡∏ü‡∏≤‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï plot‡∏Å‡∏£‡∏≤‡∏ü‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡πÄ‡∏•‡πâ‡∏ß‡πÑ‡∏î‡πâ ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç(.str.split().str.len() >= 380 & .str.split().str.len() <= 1750 & category_counts < 69)
#‡πÑ‡∏î‡πâ(0.30) 51.5%
import pandas as pd
from sklearn.model_selection import train_test_split

merged_df_filtered = merged_df[(merged_df["Translate"].str.split().str.len() >= 380) & (merged_df["Translate"].str.split().str.len() <= 1750)].copy()
category_counts = merged_df_filtered["category"].value_counts()
rare_categories = category_counts[category_counts < 69].index
merged_df_filtered = merged_df_filtered[~merged_df_filtered["category"].isin(rare_categories)].copy()
print("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:", len(merged_df_filtered), "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏°‡∏ß‡∏î:",  merged_df_filtered['category'].nunique())
print("\n‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á:")
print(category_counts)

x_train , x_test, y_train, y_test = train_test_split(merged_df_filtered["Translate"].fillna(''), merged_df_filtered["category"], test_size = 0.30, random_state = 80)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=4, stop_words='english', max_features=20000, sublinear_tf=True )
x_train_vector = vectorizer.fit_transform(x_train.fillna(''))
x_test_vector = vectorizer.transform(x_test.fillna(''))

model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(x_train_vector, y_train)


y_pred = model.predict(x_test_vector)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:", len(merged_df_filtered), "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏°‡∏ß‡∏î:",  merged_df_filtered['category'].nunique())

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

ConfusionMatrixDisplay.from_estimator(model, x_test_vector, y_test, cmap="Greens")
plt.title("Confusion Matrix - Category üòèüòëüòó")
plt.show()