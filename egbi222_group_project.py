# -*- coding: utf-8 -*-
"""EGBI222_Group_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j1xnBLIfG4mI8ymLQfJ3KEinHNPCfASU

**Set up the program to use**
"""

!pip install kaggle
from google.colab import drive
drive.mount('/content/drive/')

# 1. Remove the file (force delete, even if it's not a folder)
!rm -rf /root/.kaggle

# 2. Recreate as a directory
!mkdir -p /root/.kaggle

# 3. Copy kaggle.json from Drive
!cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle/

# 4. Fix permissions
!chmod 600 /root/.kaggle/kaggle.json

!pip install yt-dlp
!pip install -q git+https://github.com/openai/whisper.git
!apt-get install -y ffmpeg
!pip uninstall torch torchvision torchaudio -y
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install faster-whisper

!pip install -q -U google-generativeai
!pip install -q langdetect
# Import the Python SDK
import google.generativeai as genai
# Used to securely store your API key
#from google.colab import userdata
import os
#GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
genai.configure(api_key=GOOGLE_API_KEY)
model_gemini = genai.GenerativeModel('gemini-2.5-flash')
!pip install deep-translator

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import yt_dlp
import shutil
import time, random, pandas as pd, yt_dlp
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm.notebook import tqdm
import torch
from faster_whisper import WhisperModel
import whisper
import torch
from whisper.utils import get_writer
from langdetect import detect
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay

!kaggle datasets list
!kaggle datasets download cyberevil545/youtube-videos-data-for-ml-and-trend-analysis
!unzip /content/youtube-videos-data-for-ml-and-trend-analysis.zip

"""**Import file + clean file**"""

df = pd.read_csv('/content/drive/MyDrive/EGBI222_Group Project_1/youtube_data.csv')
df.sort_values(by="duration", ascending=False)
df = df[(df['duration'] > 60) & (df['duration'] < 600)]
df

"""**Programs for each machine**

P=0, Raphael=1,2 ,chen=3,ice=4
"""

#(หน้าที่) ฟาต้องเขียนเเยกข้อมูลให้เพื่อนเเต่ละคน
#ฟาในอดีตคิดว่าจะใช้Modulo แยกเเบบหารได้เท่าไหร่คนนั้นทำอันนั้นจากIndex
CSV_PATH      = "/content/drive/MyDrive/EGBI222_Group Project_1/youtube_data.csv"   # ไฟล์ข้อมูล
OUTPUT_DIR    = "/content/drive/MyDrive/EGBI222_Group Project_1/audio"  # โฟลเดอร์เก็บ .mp3
RESULTS_DIR   = "/content/drive/MyDrive/EGBI222_Group Project_1/results"  # โฟลเดอร์เก็บ CSV ผลลัพธ์

TOTAL_MACHINES = 5     # จำนวนเครื่อง(ที่ใช้รัน)
MACHINE_ID     = 1    # ลำดับเครื่อง(ใส่ค่าด้านบน).                                           #<-------------------------------------------เปลี่ยนตรงนี้

# ตั้งชื่อไฟล์ผลลัพธ์ของเครื่องนี้
SUFFIX = f"part_name"                                     #<-------------------------------------------เปลี่ยน name เป็นชื่อตัวเองตรงนี้
OUT_CSV = os.path.join(RESULTS_DIR, f"processed_{SUFFIX}.csv")

MAX_WORKERS_DL = 3
SLEEP_RANGE    = (0.5, 1.5)
COOKIE_PATH = None          #ถ้าฟาหาคุกกี้เจอเอามาเปลี่ยนตรงนี้

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

if "url" not in df.columns:
    raise ValueError( "ไม่พบคอลัมน์ 'url' ใน CSV")

# ใส่คอลัมน์ไปก่อน
for col in ["audio_path", "Transcript", "Translate"]:
    if col not in df.columns:
        df[col] = None

#แบ่งจาก index
work_df = df[df.index % TOTAL_MACHINES == MACHINE_ID].copy().reset_index(drop=True)

display(work_df)
print(f"เครื่องนี้จะประมวลผลทั้งหมด: {len(work_df)} แถว") #ต้องได้ประมาณ2200-2300

"""***Download(Video)***"""

#โค๊ดของใหม่ ก็อปมา ฟาในอดีตไม่ได้สร้างเองน้า
#ปัญหา ฟาในอดีตคิดว่ามันมันเรียกdef ทีละรอบมันโหลดทีละอันมันน่าจะช้าเกินไปนะ
#(หน้าที่) ฟาต้องเขียนอันที่ไหนกว่านี้อีกเซลล์ชื่อ(Quick Download)
#ฟาในอดีตคิดว่าdefของใหม่ดีเเล้ว เเต่ไปเปลี่ยนตอนเรียกคำสั่งให้มันโหลดหลายๆอันพร้อมกันจะได้ไวๆ อยากให้เพิ่มskipอันที่โหลดไปเเล้ว(ถ้าตัวโหลดหลายอันมันเผลอโหลดอันเดียวกันจะได้skip)
def download_audio_files(youtube_url, video_id):
    output_dir = "/content/drive/MyDrive/EGBI222_Group Project_1/audio"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    audio_file_path = os.path.join(output_dir, f'{video_id}.mp3')

    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': audio_file_path.replace('.mp3', ''),
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
        }],
        'quiet': True,
        'no_warnings': True,
    }

    print(f"🎧 Attempting to download: {youtube_url}")
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([youtube_url])
        print(f"✅ Successfully downloaded: {audio_file_path}")
        return audio_file_path
    except yt_dlp.utils.DownloadError as e:
        print(f"⚠️ ERROR downloading {youtube_url} (ID: {video_id}). Skipping file.")
        return None
    finally:
        time.sleep(2)

df_test = work_df.iloc[:5]#<------------------- เปลี่ยนตรงนี้

work_df["audio_path"] = df_test.apply(
    lambda row: download_audio_files(row['url'], row['video_id']),
    axis=1
  )

""" **Quick Download**"""

from concurrent.futures import ThreadPoolExecutor, as_completed
import yt_dlp, time, os
def download_audio_files(youtube_url, video_id):
    output_dir = "/content/drive/MyDrive/EGBI222_Group Project_1/audio"
    os.makedirs(output_dir, exist_ok=True)

    audio_file_path = os.path.join(output_dir, f"{video_id}.mp3")

    if os.path.exists(audio_file_path):
        print(f"⏩ Skip {video_id} (already exists)")
        return audio_file_path

    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': audio_file_path.replace('.mp3', ''),
        'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3'}],
        'quiet': True,
        'no_warnings': True,
    }

    try:
      #ฟาอย่าลืมใส่อีโมจิมีสีด้วยน้าจะได้เห็นง่ายๆ( ˘ ³˘)♥
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([youtube_url])
        print(f"✅ Done: {video_id}")
        return audio_file_path
    except Exception as e:
        print(f"⚠️ Error downloading {video_id}: {e}")
        return None
    finally:
        time.sleep(1) #เอาชัวโหลดหลายอันให้น้องพักบ้าง

#(หน้าที่)ฟาต้องเขียนเขียนโหลดเเบบขนานจะได้ไวขึ้น
#ปัญหา รันรัวๆไม่ได้youtubeจะหาว่าเป็นบอท(time.sleepก็ไม่ได้นะลองเเย้ว) #โดนจับประมาณตอน315-350
df_test = work_df.iloc[:300]  # ← เปลี่ยนตรงนี้ #คิดไม่ออกเเย้วโหลดมือทีละ300ไปละกันนะ  ┑(￣▽￣)┍
max_workers = 5 #5กับ10 ดีสุดจากฟาในอดีต ลองรันมาละ

futures = []
paths = [None] * len(df_test)

with ThreadPoolExecutor(max_workers=max_workers) as executor:
    for i, row in enumerate(df_test.itertuples(index=False)):
        futures.append(executor.submit(download_audio_files, row.url, row.video_id))

    for i, fut in enumerate(as_completed(futures)):
        paths[i] = fut.result()

# เขียนผลกลับเข้า work_df (เฉพาะช่วงที่โหลด)
work_df.loc[df_test.index, "audio_path"] = paths
print("ดาวน์โหลดเสร็จแล้ว🙃")

"""**##########################################################**

**Remove unloaded files**
"""

#(หน้าที่)ฟาต้องเช็คpath(ลงในcolumn["audio_path"]) ละอันไหนหาไม่เจอเเสดงว่าโหลดไม่ได้ จะได้ลบทิ้ง
def check_audio_path(video_id):

    audio_file_path = os.path.join(OUTPUT_DIR, f"{video_id}.mp3")
    if os.path.exists(audio_file_path):
        return audio_file_path
    else:
        return None


work_df["audio_path"] = work_df["video_id"].apply(check_audio_path)

work_df.dropna(subset=['audio_path'], inplace=True)
display(work_df)
print(f"เหลือที่ต้องประมวลผลทั้งหมด: {len(work_df)} แถว")

"""**#######################ห้ามกดเซลล์นี้ถ้าราฟาเอลไม่ได้สั่ง ##########################**"""

#ถ้าทำถึงขั้นนี้ทักไลน์ราฟาเอลหรือโทร0982560366 (ช่องทางอื่นอาจตอบช้า)
#(หน้าที่)เซฟไฟล์ครั้งเเรกลงdrive(จะได้ไม่หาย)*ฟาในอนาคตต้องทำอย่าคิดว่าไม่ทำก็ได้ จากฟาในอดีต
import pandas as pd, os
RESULTS_DIR = "/content/drive/MyDrive/EGBI222_Group Project_1/results"
os.makedirs(RESULTS_DIR, exist_ok=True)
OUT_CSV = os.path.join(RESULTS_DIR, "progress_name.csv") #<-------------------------------------------เปลี่ยน name เป็นชื่อตัวเองตรงนี้

work_df.to_csv(OUT_CSV, index=False, encoding="utf-8")
print(f"✅ สร้างไฟล์เริ่มต้นแล้ว -> {OUT_CSV} ({len(df)} แถว)")

"""**####################################################################**

**Load Save**
"""

#Run time out
#(หน้าที่)ฟาต้องเรียกprogress_name.csv จากdrive จะโหลดเซฟมารันต่อในกรณี มันหลุดไฟล์จะได้ไม่หาย(คอมเพื่อนจะพังตอนไหนไม่รู้เซฟไว้ก่อนดีสุด👍)
import pandas as pd, os

RESULTS_DIR = "/content/drive/MyDrive/EGBI222_Group Project_1/results"
OUT_CSV = os.path.join(RESULTS_DIR, "progress_name.csv")#<-------------------------------------------เปลี่ยน name เป็นชื่อตัวเองตรงนี้

if os.path.exists(OUT_CSV):
    work_df = pd.read_csv(OUT_CSV)
    print(f"โหลดไฟล์เดิม -> {OUT_CSV} ({len(work_df)} แถว)")
#เช็คpathเอาชัว รันในเซล์นี้เพื่อนจะได้กดเเค่อันเดียว(กลัวพอให้เพื่อนกลับไปกดอันบนละจะกดผิด🙃)
def check_audio_path(video_id):
    Merged = "/content/drive/MyDrive/EGBI222_Group Project_1/audio"
    audio_file_path = os.path.join(Merged, f"{video_id}.mp3")
    if os.path.exists(audio_file_path):
        return audio_file_path
    else:
        return None
work_df["audio_path"] = work_df["video_id"].apply(check_audio_path)
work_df.dropna(subset=['audio_path'], inplace=True)
display(work_df)

"""**Transcipt+Translate**"""

#(หน้าที่)ฟาต้องเขียนTranscipt+Translate รีเทรินเป็นข้อความ
device = "cuda" if torch.cuda.is_available() else "cpu"
compute = "float16" if device == "cuda" else "int8"
MODEL_SIZE = "small"           #เปลี่ยนโมเดลTransciptตรงนี้
model = WhisperModel(MODEL_SIZE, device=device, compute_type=compute)

def transcribe_one(path: str) -> str:
    segments, _ = model.transcribe(
        path,
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=500), #เวลาจบประโยค แบบ 100 คือถ้าเงียบ100ms ให้จบประโยคนั้น 500 ดีสุดจากฟาในอดีต ลองรันมาละ
        beam_size=1, best_of=1,
        without_timestamps=True,
        word_timestamps=False,
        condition_on_previous_text=False,
        temperature=0
    )
    return "".join(s.text for s in segments).strip()

def translate_one(path: str) -> str:
    # Whisper translate แปล "เป็นภาษาอังกฤษ" เท่านั้น (task=translate)
    segments, _ = model.transcribe(
        path,
        task="translate",
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=500), #แปลจากข้อความไม่ต้องใส่ก็ได้(ก็อปมาจากtranscribe🙃)
        beam_size=1, best_of=1,
        without_timestamps=True,
        word_timestamps=False,
        condition_on_previous_text=False,
        temperature=0
    )
    return "".join(s.text for s in segments).strip()
#import ใหม่ colab มันเป็นไรไม่รู้😭
import os, re
from tqdm.notebook import tqdm
#(หน้าที่)ฟาต้องเขียนตรวจประโยคเลยถ้าคลิปมันไม่มีบทพูดก็ลบทิ้งเลยจะได้ไม่ต้องไปcleanทีหลัง(ML เดาCategoryจากtranslate(คลิปไม่มีบทพูดมันจะเดามั่ว))
def is_valid_sentence(s: str, min_chars=10, min_words=2) -> bool:
    if not isinstance(s, str):
        return False
    s = s.strip()
    if not s:
        return False
    if s.upper().startswith("ERROR"):
        return False
    if len(s) < min_chars:
        return False
    # นับคำคร่าว ๆ (ถ้าไม่มีการเว้นวรรคเลย จะถือว่าผ่านเฉพาะกรณีข้อความยาวพอ) (ประโยคน้อยไปจะทำให้ML เดายากละเดาผิด)
    words = re.findall(r'\w+', s, flags=re.UNICODE)
    if len(words) < min_words and len(s) < (min_chars * 1.5):
        return False
    return True

#(หน้าที่)ฟาต้องเขียนรันเเบบรันไปเซฟไป ไฟล์จะได้ไม่หาย
#ฟาในอดีตคิดว่าใช้while loop เเบบรันไปเรื่อยๆเลยจนหมด ถ้าโปรแกรมพังก็ไปรันตรงRun time out ใหม่ละให้อันนี้รันต่อจากอันล่าสุดเลย
import time
from tqdm.notebook import tqdm
import os

BATCH_SIZE = 10 #เซฟทุกๆกี่อัน น้อยไปก็ช้า มากไปถ้าพังก็ร้องอ่ะ

while True:

    pending = work_df[work_df["Transcript"].isna() | (work_df["Transcript"].astype(str).str.strip() == "")]
    if len(pending) == 0:
        print("ไม่มีแถวที่เหลือ เสร็จหมดแล้ว༼ ◔ ͜ʖ ◔ ༽")
        break

    # เลือก batch 10 แถวแรกจากที่ยังเหลือ
    df_subset = pending.iloc[:BATCH_SIZE]
    print(f"เริ่มทำ {len(df_subset)} แถว จากที่เหลือ {len(pending)} แถว")

    pbar = tqdm(total=len(df_subset), desc="กำลังTranscipt+Translate", unit="file")
    drop_indices = [] #เก็บอันที่เสียละลบทีเดียว(ไวกว่าเจอละลบๆ)O(1)กับO(n)
    for i, row in df_subset.iterrows():
        path = row["audio_path"]

        if not isinstance(path, str) or not os.path.exists(path):
            drop_indices.append(i)
            pbar.update(1)
            continue

        # ถอดเสียง
        txt = transcribe_one(path)
        if not is_valid_sentence(txt):
            drop_indices.append(i)
            pbar.update(1)
            continue

        # ถอดได้ → บันทึกลง df
        work_df.at[i, "Transcript"] = txt

        # แปล
        trans = translate_one(path)
        work_df.at[i, "Translate"] = trans

        pbar.update(1)

    if drop_indices:
        work_df.drop(index=drop_indices, inplace=True)
        work_df.reset_index(drop=True, inplace=True)
        print(f"ลบ {len(drop_indices)} แถวที่ไม่ผ่าน")

    pbar.close()

    # ✅ เซฟต่อไฟล์เดิม
    work_df.to_csv(OUT_CSV, index=False, encoding="utf-8")
    print(f"บันทึกต่อไฟล์เดิม -> {OUT_CSV}")

    # ---- พักระหว่าง batch ----
    time.sleep(3) #ให้น้องพักหน่อยเอาชัวร์

display(work_df)

"""**Save**


"""

#เซฟครั้งสุดท้ายเอาชัวร์
work_df.to_csv(OUT_CSV, index=False, encoding="utf-8")
print(f"✅ บันทึกผลแล้ว: {OUT_CSV}")

"""**Merge files**

*   ส่งไฟล์ลงในDrive results ของราฟา (ส่งเเล้วบอกราฟาด้วย)
*   ไม่ต้องทำหลังจากนี้เเล้ว(ไม่ต้องรันหลังจากนี้เเล้ว)
"""

from google.colab import drive
drive.mount('/content/drive/')

#(หน้าที่)ฟาต้องรวมไฟล์ ละเอาไฟล์ไปที่ folder Master
import pandas as pd
import glob

# เส้นทางโฟลเดอร์
path = "/content/drive/MyDrive/EGBI222_Group Project_1/results"
Master_path = "/content/drive/MyDrive/EGBI222_Group Project_1/Master"

all_files = glob.glob(path + "/*.csv")
dfs = [pd.read_csv(f) for f in all_files]
merged_df = pd.concat(dfs, ignore_index=True)
merged_df = merged_df.drop_duplicates(keep='last')
# ดูขนาดไฟล์รวม
print("รวมแล้วได้ทั้งหมด", len(merged_df), "แถว")

# บันทึกเป็นไฟล์ใหม่
merged_df.to_csv(Master_path + "/Master.csv", index=False)
display(merged_df)
merged_df.shape

#(หน้าที่)ฟาต้องclean file ละสร้างไฟล์ใหม่(จะได้ไม่ต้องทับอันเก่า)
import pandas as pd

# โหลดไฟล์
path = "/content/drive/MyDrive/EGBI222_Group Project_1/Master/Master.csv"
df = pd.read_csv(path)

# ดูข้อมูลก่อนทำ
print("ก่อนทำความสะอาด:", df.shape)
print(df.columns)

df = df.dropna(subset=['Translate', 'category'])
df = df[
    (df['Transcript'].astype(str).str.strip() != '') &
    (df['Translate'].astype(str).str.strip() != '') &
    (df['category'].astype(str).str.strip() != '')
]

df['Translate'] = df['Translate'].astype(str).str.strip()
df['category'] = df['category'].astype(str).str.strip()

df = df.drop_duplicates(subset=['Transcript', 'Translate', 'category'], keep='last')
#รีเซ็ต index
df = df.reset_index(drop=True)
# บันทึกไฟล์ใหม่
clean_path = "/content/drive/MyDrive/EGBI222_Group Project_1/Master/Master_clean.csv"
df.to_csv(clean_path, index=False)

print("หลังทำความสะอาด:", df.shape)
print("บันทึกไฟล์แล้วที่:", clean_path)

"""**Train ML**"""

#(หน้าที่)ฟาต้องเลือกข้อมูลที่ดีที่สุดมาtrain เพราะเดาcategoryจากเเค่คำพูดเท่านั้น ไม่มีน้ำเสียง เว้นระยะการพูด หรือการกระทำ จากข้อมูลจำนวนตัวอย่าง: 5380 จำนวนหมวด: 16
#ฟาในอดีตคิดว่า จะตัดcategory น้อยๆออก ละรวมเข้าother
#ฟาในอดีตรวมother เเล้วML มันไปตอบotherซะเยอะเลย
#ฟาในอดีตคิดว่าตัดออกไปเลยดีกว่า
#ฟาในอดีตลองตัดเเล้วคิดว่า 6 ได้%มากสุด เเต่หมวดจะเหลือน้อยไป จาก16หมวด จากมากที่สุดควรไม่น้อยกว่า8
#ฟาในอดีตลองครบทุกหมวดเเล้วtrain จากนั้นplot กราฟเเล้ว8-9หมวดดีสุด เพราะหลังจากนั้นกราฟเริ่มลงเเล้ว
#ฟาในอดีตลองชุดข้อมูลเเล้วtrain จากนั้นplot กราฟเเล้ว ข้อมูลประมาณ2500-3200 ดีที่สุด
#ฟาในอดีตหาจุดสมดุลของชุดข้อมูลกับหมวดไม่ได้ พอรวมกันเเล้วสูงสุดอยู่ที่38%
#ฟาในอดีตลองเปลี่ยนML สรุปสูงสุด อยู่ที่ RoBARTa(DistilBERT) 41.2%(จากการลอง linear ,TF-IDF Vectorizer(LogisticRegression), solver,DistilBERT,MiniLM(,MiniLM-L16))
#ฟาในอดีตคิดว่าจะเทรนML เเล้วใช้MLตรวจข้อมูลที่ใช้trainซ้อนอีกที เพื่อหาข้อมูลBest of 1000rowที่มีอยู่แล้วทำให้MLมีaccสูงที่สุด ละเอาชุดข้อมูลใหม่นั้นมาเทรนMLใหม่
#Best of 1000row อยู่ในrange 315ขึ้นไปพอดี(ใช้คำสั่ง.str.split().str.len() >= 315 ได้เลย) โดยข้อมูลที่เยอะที่สุดอยู่ที่ 1350 โดย หมวดอยู่ที่ เเต่ละหมวด มากกว่า60 ตัวอย่าง
#สูงสุดอยู่ที่ LogisticRegression 43.2%
#ฟาในอดีต ว่าจะลองหาRange best of 1000 ของเเต่ละmodel ไปเลย
#ฟาในอดีตลองหาเเล้ว นำมายูเนี่ยนกันมีBest of 1000 ประมาณ1078 อันที่ซ้ำกันมากกว่า2model
#ฟาในอดีตลองเอา1078นั้นมารันละได้   LogisticRegression 44.1%
#ฟาในอดีตลองหาtest_size ที่ดีที่สุด(โดยมากกว่า0.2)ของเเต่ละmodel จากนั้นplot กราฟเเล้ว(นานมาก16ชม. (─‿‿─) )
#ได้linear(0.28-0.33(แต่accต่ำมาก(ต่ำกว่า30%))) ,TF-IDF Vectorizer(LogisticRegression)(0.3-0.33), solver(0.27-0.31),DistilBERT(0.24-0.26),MiniLM(,MiniLM-L16(0.29-0.32)) สูงสุดอยู่ที่  LogisticRegression 44.1%(0.33)
#ฟาในอดีต จะลองหาชุดข้อมูลจาก1078 0kd ใช้MLตรวจข้อมูลข้อมูลที่ใช้trainซ้อนอีกที มาใส่ในLogisticRegression (0.33)ดู
#ลองได้998 ชุดคำสั่งเงื่อนไข(.str.split().str.len() >= 315 & category_counts < 70) อยู่ที่  LogisticRegression 49%(0.33) (อันอื่นเทียบไม่ติดเลย) (หลังจากนี้จะไม่ใช้อันอื่นอีกละ เพื่อลดเวลาการทำ(จริงๆขก.ละอ่ะเหนื่อย))
#ฟาในอดีตจะลอง หาจุดสมดุลของชุดข้อมูลกับหมวด เเล้วพล็อตกราฟดู(นานมาก11ชม. (─‿‿─) )(ท้อเเย้วน้าาาาาา(•﹏•) )
#ได้1023ชุดคำสั่งเงื่อนไข(.str.split().str.len() >= 315  .str.split().str.len() <= 1750 & category_counts < 63) (จริงๆ1026 เเต่หาเงื่อนไขไม่ได้(จริงๆยัดเพิ่มได้นะเเต่ขก.'◡' ))
#ลอง LogisticRegression 51.1%
#ฟาในอดีตลองหาtest_size ที่ดีที่สุด(โดยมากกว่า0.2)ของLogisticRegression กับชุดข้อมูลนี้ดูจากนั้นplotกราฟหาสูงสุด((ขนาดอันเดียวยังเกือบ4ชม. (─‿‿─) ))
#ได้0.27-0.35
#ฟาในอดีต ลองละได้0.31 51.5%
#ฟาในอดีตลอง 1026 ชุดมาเทรน ละได้น้อยกว่าเดิม 48.7% (ลองกดๆมือหาละอันนี้มากสุดละ(ขก.รันพล็อตกราฟเเย้วนานเกิน))
#ฟาในอดีตกลับมาใช้ขอ้มูลเดิมที่ได้ 51.5%
#ฟาในอดีต plotกราฟข้อมูลเเล้วได้ ชุดข้อมูลเงื่อนไข(.str.split().str.len() >= 380 & .str.split().str.len() <= 1750 & category_counts < 69)
#ได้(0.30) 51.5%
import pandas as pd
from sklearn.model_selection import train_test_split

merged_df_filtered = merged_df[(merged_df["Translate"].str.split().str.len() >= 380) & (merged_df["Translate"].str.split().str.len() <= 1750)].copy()
category_counts = merged_df_filtered["category"].value_counts()
rare_categories = category_counts[category_counts < 69].index
merged_df_filtered = merged_df_filtered[~merged_df_filtered["category"].isin(rare_categories)].copy()
print("จำนวนตัวอย่าง:", len(merged_df_filtered), "จำนวนหมวด:",  merged_df_filtered['category'].nunique())
print("\nจำนวนแต่ละหมวดหมู่หลังจากกรอง:")
print(category_counts)

x_train , x_test, y_train, y_test = train_test_split(merged_df_filtered["Translate"].fillna(''), merged_df_filtered["category"], test_size = 0.30, random_state = 80)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=4, stop_words='english', max_features=20000, sublinear_tf=True )
x_train_vector = vectorizer.fit_transform(x_train.fillna(''))
x_test_vector = vectorizer.transform(x_test.fillna(''))

model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(x_train_vector, y_train)


y_pred = model.predict(x_test_vector)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("จำนวนตัวอย่าง:", len(merged_df_filtered), "จำนวนหมวด:",  merged_df_filtered['category'].nunique())

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

ConfusionMatrixDisplay.from_estimator(model, x_test_vector, y_test, cmap="Greens")
plt.title("Confusion Matrix - Category 😏😑😗")
plt.show()